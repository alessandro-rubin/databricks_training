{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsgud0/5MEz5KCZnvBUAUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandro-rubin/databricks_training/blob/main/Find_consecutive_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIM: to find samples with consecutive values of a signal in a dataframe with signals from many different vehicles"
      ],
      "metadata": {
        "id": "_7jf1q2CsWiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHX729QTe1Qp",
        "outputId": "3b5b7546-c221-4fef-a895-92791397e948"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=7003ac0cd80428915ff39387a3b2bebd70cd7dbb07598dac0bc0b171622c0917\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7qOArClemNk",
        "outputId": "8517c5ae-4422-42af-e919-3c1ff7be7499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('vehicle_2', 'v_0', '2023-09-12 08:03:00'),)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lag, lead\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def find_consecutive_intervals(df, signal_column, v_0, n):\n",
        "    # Define a window specification to order rows by datetime\n",
        "    window_spec = Window.orderBy(\"datetime\")\n",
        "\n",
        "    # Create a column that indicates when the signal is equal to v_0\n",
        "    df = df.withColumn(\"is_v_0\", (F.col(signal_column) == v_0).cast(\"integer\"))\n",
        "\n",
        "    # Create a column that assigns a group ID to consecutive rows with the same is_v_0 value\n",
        "    df = df.withColumn(\n",
        "        \"group_id\",\n",
        "        F.sum(\"is_v_0\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
        "    )\n",
        "\n",
        "    # Create a column that counts the number of consecutive v_0 rows within each group\n",
        "    df = df.withColumn(\n",
        "        \"consecutive_count\",\n",
        "        F.when(F.col(\"is_v_0\") == 1, F.sum(\"is_v_0\").over(window_spec)).otherwise(0)\n",
        "    )\n",
        "    df.orderBy('vehicle','datetime').show()\n",
        "\n",
        "    # Filter rows where consecutive_count is greater than or equal to n\n",
        "    filtered_df = df.filter(F.col(\"consecutive_count\") >= n)\n",
        "\n",
        "    # Calculate the start and end of each interval and interval length in samples\n",
        "    result_df = filtered_df.groupBy(\"group_id\").agg(\n",
        "        F.min(\"datetime\").alias(\"start_datetime\"),\n",
        "        F.max(\"datetime\").alias(\"end_datetime\"),\n",
        "        F.count(\"*\").alias(\"interval_length_samples\")\n",
        "    )\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def find_consecutive_intervals2(df, signal_column, v_0, n):\n",
        "    # Define a window specification to order rows by datetime\n",
        "    window_spec = Window.orderBy(\"datetime\")\n",
        "\n",
        "    # Create a column that indicates when the signal is equal to v_0\n",
        "    df = df.withColumn(\"is_v_0\", (F.col(signal_column) == v_0).cast(\"integer\"))\n",
        "\n",
        "    # Create a column that assigns a group ID to consecutive rows with the same is_v_0 value\n",
        "    df = df.withColumn(\n",
        "        \"group_id\",\n",
        "        -F.sum(\"is_v_0\").over(window_spec) + F.row_number().over(window_spec)\n",
        "    )\n",
        "\n",
        "    # Create a column that counts the number of consecutive v_0 rows within each group\n",
        "    df = df.withColumn(\n",
        "        \"consecutive_count\",\n",
        "        F.when(F.col(\"is_v_0\") == 1, F.sum(\"is_v_0\").over(Window.partitionBy('vehicle',\"group_id\"))).otherwise(0)\n",
        "    )\n",
        "    df.orderBy('vehicle','datetime').show()\n",
        "\n",
        "    # Filter rows where consecutive_count is greater than or equal to n\n",
        "    filtered_df = df.filter(F.col(\"consecutive_count\") >= n)\n",
        "    filtered_df.show()\n",
        "    # Calculate the start and end of each interval and interval length in samples\n",
        "    result_df = filtered_df.groupBy(\"group_id\").agg(\n",
        "        F.min(\"datetime\").alias(\"start_datetime\"),\n",
        "        F.max(\"datetime\").alias(\"end_datetime\"),\n",
        "        F.count(\"*\").alias(\"interval_length_samples\"),\n",
        "        F.first('vehicle')\n",
        "    )\n",
        "\n",
        "    return result_df\n",
        "# Usage example:\n",
        "# Assuming 'df' is your PySpark DataFrame\n",
        "columns=[\"vehicle\", \"signal\", \"datetime\"]\n",
        "\n",
        "data = [\n",
        "        (\"vehicle_2\", \"v_0\", \"2023-09-12 08:00:00\"),\n",
        "        (\"vehicle_2\", \"v_1\", \"2023-09-12 08:01:00\"),\n",
        "        (\"vehicle_2\", \"v_0\", \"2023-09-12 08:03:00\"),\n",
        "        (\"vehicle_2\", \"v_0\", \"2023-09-12 08:02:00\"),\n",
        "        (\"vehicle_2\", \"v_0\", \"2023-09-12 08:03:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:00:00\"),\n",
        "        (\"vehicle_1\", \"v_1\", \"2023-09-12 08:01:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:02:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:03:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:04:00\"),\n",
        "        (\"vehicle_2\", \"v_1\", \"2023-09-12 08:05:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:06:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:07:00\"),\n",
        "        (\"vehicle_1\", \"v_0\", \"2023-09-12 08:08:00\"),\n",
        "    ]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "n = 3  # Minimum consecutive rows with the same signal value\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows=100\n",
        "\n",
        "np.random.randint(1,26,n_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PptljNbnRZo",
        "outputId": "48fff8bb-85a3-44d1-b612-b8e55c8ddd63"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14,  6,  9, 24,  1,  7,  5, 16,  6,  3,  3, 25, 23, 21,  9, 13,  9,\n",
              "       21, 11, 21, 17, 21,  5, 20, 21, 12,  2, 24,  6, 12, 18, 16, 21,  4,\n",
              "        4, 21, 21,  5, 12, 13, 15,  2, 14,  1, 11, 18, 12, 13,  3, 12, 16,\n",
              "        9, 11,  1, 10,  9,  8,  1,  9,  4,  7,  6, 24, 21, 20,  6,  4,  2,\n",
              "        6,  7,  6,  6,  8, 15,  1,  2, 17, 14,  3,  1,  9, 11, 22,  7, 16,\n",
              "       21, 21, 13, 19, 13,  5,  5, 20, 18, 15,  7, 23, 23, 20, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "result = find_consecutive_intervals(df,'signal','v_0', n)\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG2rRMQZjo7p",
        "outputId": "74fcb6ce-963d-4e68-ad53-bf7381ce99f3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "|  vehicle|signal|           datetime|is_v_0|group_id|consecutive_count|\n",
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "|vehicle_1|   v_0|2023-09-12 08:00:00|     1|       1|                1|\n",
            "|vehicle_1|   v_1|2023-09-12 08:01:00|     0|       1|                0|\n",
            "|vehicle_1|   v_0|2023-09-12 08:02:00|     1|       2|                2|\n",
            "|vehicle_1|   v_0|2023-09-12 08:03:00|     1|       3|                3|\n",
            "|vehicle_1|   v_0|2023-09-12 08:04:00|     1|       4|                4|\n",
            "|vehicle_2|   v_1|2023-09-12 08:05:00|     0|       4|                0|\n",
            "|vehicle_1|   v_0|2023-09-12 08:06:00|     1|       5|                5|\n",
            "|vehicle_1|   v_0|2023-09-12 08:07:00|     1|       6|                6|\n",
            "|vehicle_1|   v_0|2023-09-12 08:08:00|     1|       7|                7|\n",
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "|group_id|     start_datetime|       end_datetime|interval_length_samples|\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "|       3|2023-09-12 08:03:00|2023-09-12 08:03:00|                      1|\n",
            "|       4|2023-09-12 08:04:00|2023-09-12 08:04:00|                      1|\n",
            "|       5|2023-09-12 08:06:00|2023-09-12 08:06:00|                      1|\n",
            "|       6|2023-09-12 08:07:00|2023-09-12 08:07:00|                      1|\n",
            "|       7|2023-09-12 08:08:00|2023-09-12 08:08:00|                      1|\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_consecutive_intervals(df,'signal','v_0', n)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZnOp5Tksxp",
        "outputId": "9890b044-83b4-47d6-a5be-8dedb7bb90ac"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "|  vehicle|signal|           datetime|is_v_0|group_id|consecutive_count|\n",
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "|vehicle_1|   v_0|2023-09-12 08:00:00|     1|       2|                2|\n",
            "|vehicle_1|   v_1|2023-09-12 08:01:00|     0|       2|                0|\n",
            "|vehicle_1|   v_0|2023-09-12 08:02:00|     1|       4|                4|\n",
            "|vehicle_1|   v_0|2023-09-12 08:03:00|     1|       6|                6|\n",
            "|vehicle_1|   v_0|2023-09-12 08:04:00|     1|       8|                8|\n",
            "|vehicle_1|   v_0|2023-09-12 08:06:00|     1|       9|                9|\n",
            "|vehicle_1|   v_0|2023-09-12 08:07:00|     1|      10|               10|\n",
            "|vehicle_1|   v_0|2023-09-12 08:08:00|     1|      11|               11|\n",
            "|vehicle_2|   v_0|2023-09-12 08:00:00|     1|       1|                2|\n",
            "|vehicle_2|   v_1|2023-09-12 08:01:00|     0|       2|                0|\n",
            "|vehicle_2|   v_0|2023-09-12 08:02:00|     1|       3|                4|\n",
            "|vehicle_2|   v_0|2023-09-12 08:03:00|     1|       5|                6|\n",
            "|vehicle_2|   v_0|2023-09-12 08:03:30|     1|       7|                7|\n",
            "|vehicle_2|   v_1|2023-09-12 08:05:00|     0|       8|                0|\n",
            "+---------+------+-------------------+------+--------+-----------------+\n",
            "\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "|group_id|     start_datetime|       end_datetime|interval_length_samples|\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "|       3|2023-09-12 08:02:00|2023-09-12 08:02:00|                      1|\n",
            "|       4|2023-09-12 08:02:00|2023-09-12 08:02:00|                      1|\n",
            "|       5|2023-09-12 08:03:00|2023-09-12 08:03:00|                      1|\n",
            "|       6|2023-09-12 08:03:00|2023-09-12 08:03:00|                      1|\n",
            "|       7|2023-09-12 08:03:30|2023-09-12 08:03:30|                      1|\n",
            "|       8|2023-09-12 08:04:00|2023-09-12 08:04:00|                      1|\n",
            "|       9|2023-09-12 08:06:00|2023-09-12 08:06:00|                      1|\n",
            "|      10|2023-09-12 08:07:00|2023-09-12 08:07:00|                      1|\n",
            "|      11|2023-09-12 08:08:00|2023-09-12 08:08:00|                      1|\n",
            "+--------+-------------------+-------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}