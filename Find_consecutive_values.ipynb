{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrskzZSaoef2hiLUCXsrzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandro-rubin/databricks_training/blob/main/Find_consecutive_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIM: to find samples with consecutive values of a signal in a dataframe with signals from many different vehicles"
      ],
      "metadata": {
        "id": "_7jf1q2CsWiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHX729QTe1Qp",
        "outputId": "5b1ecbff-2db3-4b97-eb7d-2ec31d71183b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=e94872ad9b56f039025b3bb9e129b90d3f0b5bcee932e792690529ba3089b518\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "d7qOArClemNk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lag, lead\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "\n",
        "def find_consecutive_intervals(df, signal_column, v_0, n):\n",
        "    # Define a window specification to order rows by datetime\n",
        "    window_spec = Window.orderBy(\"datetime\")\n",
        "\n",
        "    # Create a column that indicates when the signal is equal to v_0\n",
        "    df = df.withColumn(\"is_v_0\", (F.col(signal_column) == v_0).cast(\"integer\"))\n",
        "\n",
        "    # Create a column that assigns a group ID to consecutive rows with the same is_v_0 value\n",
        "    df = df.withColumn(\n",
        "        \"group_id\",\n",
        "        F.sum(\"is_v_0\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
        "    )\n",
        "\n",
        "    # Create a column that counts the number of consecutive v_0 rows within each group\n",
        "    df = df.withColumn(\n",
        "        \"consecutive_count\",\n",
        "        F.when(F.col(\"is_v_0\") == 1, F.sum(\"is_v_0\").over(window_spec)).otherwise(0)\n",
        "    )\n",
        "    df.orderBy('vehicle','datetime').show()\n",
        "\n",
        "    # Filter rows where consecutive_count is greater than or equal to n\n",
        "    filtered_df = df.filter(F.col(\"consecutive_count\") >= n)\n",
        "\n",
        "    # Calculate the start and end of each interval and interval length in samples\n",
        "    result_df = filtered_df.groupBy(\"group_id\").agg(\n",
        "        F.min(\"datetime\").alias(\"start_datetime\"),\n",
        "        F.max(\"datetime\").alias(\"end_datetime\"),\n",
        "        F.count(\"*\").alias(\"interval_length_samples\")\n",
        "    )\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def find_consecutive_intervals2(df, signal_column,vehicle_col,time_col, v_0, n):\n",
        "    # Define a window specification to order rows by datetime\n",
        "    window_spec = Window.orderBy(time_col)\n",
        "\n",
        "    # Create a column that indicates when the signal is equal to v_0\n",
        "    df = df.withColumn(\"is_v_0\", (F.col(signal_column) == v_0).cast(\"integer\"))\n",
        "\n",
        "    # Create a column that assigns a group ID to consecutive rows with the same is_v_0 value\n",
        "    df = df.withColumn(\n",
        "        \"group_id\",\n",
        "        -F.sum(\"is_v_0\").over(window_spec) + F.row_number().over(window_spec)\n",
        "    )\n",
        "\n",
        "    # Create a column that counts the number of consecutive v_0 rows within each group\n",
        "    df = df.withColumn(\n",
        "        \"consecutive_count\",\n",
        "        F.when(F.col(\"is_v_0\") == 1, F.sum(\"is_v_0\").over(Window.partitionBy(vehicle_col,\"group_id\"))).otherwise(0)\n",
        "    )\n",
        "    df.orderBy(vehicle_col,'datetime').show()\n",
        "\n",
        "    # Filter rows where consecutive_count is greater than or equal to n\n",
        "    filtered_df = df.filter(F.col(\"consecutive_count\") >= n)\n",
        "    filtered_df.show()\n",
        "    # Calculate the start and end of each interval and interval length in samples\n",
        "    result_df = filtered_df.groupBy(\"group_id\").agg(\n",
        "        F.min(time_col).alias(\"start_datetime\"),\n",
        "        F.max(time_col).alias(\"end_datetime\"),\n",
        "        F.count(\"*\").alias(\"interval_length_samples\"),\n",
        "        F.first(vehicle_col)\n",
        "    )\n",
        "\n",
        "    return result_df\n",
        "# Usage example:\n",
        "# Assuming 'df' is your PySpark DataFrame\n",
        "columns=[\"vehicle\", \"signal\", \"datetime\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows=1000\n",
        "veh=[f'veh_{i}' for i in np.random.randint(1,3,n_rows)]\n",
        "signal=np.maximum(np.random.randint(1,15,n_rows),10)\n",
        "signal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PptljNbnRZo",
        "outputId": "b59950ca-7fe0-4bb1-de10-cae2e1e27cd0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 13, 10, 10, 11, 10, 12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "       13, 10, 10, 10, 10, 11, 13, 10, 11, 10, 10, 11, 10, 10, 10, 13, 12,\n",
              "       10, 10, 14, 10, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 14, 10, 10,\n",
              "       10, 10, 10, 10, 10, 12, 10, 10, 10, 10, 12, 10, 14, 10, 10, 11, 10,\n",
              "       14, 10, 10, 10, 10, 10, 10, 10, 14, 13, 10, 10, 14, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 14, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "       10, 12, 10, 10, 10, 13, 10, 14, 10, 10, 13, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 11, 10, 11, 11, 10, 10, 10, 10, 11, 10, 10, 10, 11, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 13, 10, 10, 11, 10, 10, 14, 11, 10, 10, 14,\n",
              "       10, 10, 12, 10, 10, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 13, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 11, 13, 10, 14, 10, 10, 10, 13, 10, 10, 12,\n",
              "       10, 12, 13, 10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 11, 10, 10,\n",
              "       10, 10, 10, 10, 10, 14, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 13, 10, 10, 10, 10, 10, 11, 10, 12, 14, 10,\n",
              "       10, 10, 12, 10, 14, 10, 10, 10, 13, 10, 10, 10, 11, 11, 10, 11, 14,\n",
              "       10, 10, 10, 10, 12, 10, 10, 12, 10, 10, 10, 13, 14, 10, 10, 11, 10,\n",
              "       10, 10, 10, 14, 10, 14, 10, 13, 10, 10, 10, 10, 10, 10, 14, 10, 10,\n",
              "       10, 10, 11, 13, 10, 10, 10, 10, 10, 12, 13, 10, 10, 10, 10, 14, 10,\n",
              "       10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 13, 10, 10, 14, 10,\n",
              "       10, 12, 13, 12, 10, 10, 10, 10, 13, 13, 12, 10, 10, 11, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 10, 14, 10, 11, 10, 10, 13, 13, 14, 12, 10,\n",
              "       10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 11, 10, 10, 10, 14, 10, 10,\n",
              "       10, 10, 12, 10, 14, 11, 13, 12, 10, 10, 10, 13, 10, 10, 10, 10, 14,\n",
              "       10, 11, 12, 10, 10, 14, 11, 10, 10, 10, 10, 10, 10, 10, 10, 14, 13,\n",
              "       10, 12, 10, 10, 10, 11, 10, 11, 10, 11, 13, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 14, 10, 10, 12, 11, 14, 10,\n",
              "       11, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 10, 13, 10, 10, 10, 14,\n",
              "       10, 10, 10, 10, 12, 10, 10, 10, 10, 10, 14, 10, 10, 13, 13, 10, 10,\n",
              "       11, 12, 10, 10, 12, 10, 12, 10, 11, 10, 10, 10, 12, 12, 10, 12, 11,\n",
              "       10, 10, 10, 10, 11, 10, 13, 10, 10, 10, 13, 10, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 12, 11, 13, 14, 10, 10, 12, 10, 10, 11, 10, 13,\n",
              "       10, 10, 10, 10, 10, 10, 11, 14, 10, 10, 10, 13, 10, 10, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 13, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 14,\n",
              "       14, 10, 10, 10, 10, 13, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "       10, 11, 12, 10, 10, 10, 10, 10, 10, 10, 11, 10, 14, 11, 13, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 14, 10, 10, 13, 12, 13, 10, 10, 13, 10, 10,\n",
              "       10, 10, 10, 10, 11, 10, 14, 12, 10, 11, 10, 10, 10, 13, 10, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 13, 10, 10, 11, 14, 10, 10,\n",
              "       10, 10, 12, 10, 10, 10, 11, 13, 13, 10, 10, 10, 10, 11, 13, 10, 10,\n",
              "       10, 10, 10, 10, 10, 10, 10, 13, 13, 14, 10, 10, 10, 10, 10, 11, 13,\n",
              "       12, 10, 10, 10, 10, 12, 11, 10, 10, 10, 11, 12, 14, 12, 10, 10, 10,\n",
              "       10, 10, 10, 10, 11, 14, 10, 10, 12, 10, 10, 10, 10, 12, 10, 10, 10,\n",
              "       10, 10, 13, 10, 11, 10, 10, 11, 10, 10, 11, 10, 10, 10, 11, 10, 13,\n",
              "       12, 13, 10, 10, 10, 10, 10, 14, 10, 10, 10, 10, 10, 10, 13, 10, 10,\n",
              "       10, 10, 13, 10, 12, 10, 10, 10, 10, 13, 10, 10, 12, 10, 10, 10, 10,\n",
              "       10, 10, 14, 10, 10, 10, 10, 10, 12, 12, 10, 10, 10, 10, 12, 12, 10,\n",
              "       10, 10, 10, 11, 13, 10, 10, 10, 12, 10, 10, 10, 10, 10, 10, 13, 14,\n",
              "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 10, 10, 10, 14, 13, 10,\n",
              "       10, 10, 10, 14, 12, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 11, 10,\n",
              "       10, 10, 11, 10, 11, 10, 10, 14, 10, 13, 10, 10, 10, 13, 10, 10, 10,\n",
              "       10, 14, 14, 10, 13, 10, 10, 10, 14, 10, 10, 12, 10, 10, 10, 11, 10,\n",
              "       11, 10, 10, 10, 13, 13, 10, 10, 10, 10, 10, 13, 10, 14, 10, 10, 10,\n",
              "       10, 13, 12, 10, 12, 10, 10, 10, 14, 10, 10, 10, 10, 10, 10, 12, 10,\n",
              "       10, 10, 14, 10, 10, 12, 10, 10, 11, 13, 10, 10, 10, 10, 10, 13, 10,\n",
              "       13, 10, 10, 10, 11, 14, 10, 14, 12, 10, 14, 10, 14, 10, 10, 10, 12,\n",
              "       10, 10, 13, 11, 10, 10, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11,\n",
              "       10, 10, 10, 13, 10, 10, 10, 11, 10, 10, 14, 10, 10, 10, 13, 10, 10,\n",
              "       10, 10, 14, 10, 14, 10, 12, 10, 11, 11, 10, 13, 12, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datelist = pd.date_range(datetime.today(), periods=n_rows,freq='S').tolist()\n"
      ],
      "metadata": {
        "id": "xif0lw_vw7qi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(veh[i],float(signal[i]),datetime.strftime(datelist[i],'%Y-%m-%d %H:%M:%S') ) for i in range(n_rows) ]"
      ],
      "metadata": {
        "id": "MWZCHD8VyduC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "n=7"
      ],
      "metadata": {
        "id": "Y8p5FM76xnNR",
        "outputId": "befe4301-ffed-44a1-c37e-3931ae5fdbe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------------------+\n",
            "|vehicle|signal|           datetime|\n",
            "+-------+------+-------------------+\n",
            "|  veh_1|  10.0|2023-09-13 18:32:27|\n",
            "|  veh_1|  13.0|2023-09-13 18:32:28|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:29|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:30|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:31|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:32|\n",
            "|  veh_1|  12.0|2023-09-13 18:32:33|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:34|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:35|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:36|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:37|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:38|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:39|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:40|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:41|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:42|\n",
            "|  veh_2|  10.0|2023-09-13 18:32:43|\n",
            "|  veh_2|  13.0|2023-09-13 18:32:44|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:45|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:46|\n",
            "+-------+------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_consecutive_intervals(df,'signal',6, n)\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG2rRMQZjo7p",
        "outputId": "15f11dd4-e065-4985-8f7a-67d3c1e4df04"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|vehicle|signal|           datetime|is_v_0|group_id|consecutive_count|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|  veh_1|  10.0|2023-09-13 18:32:27|     0|       0|                0|\n",
            "|  veh_1|  13.0|2023-09-13 18:32:28|     0|       0|                0|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:31|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:32|     0|       0|                0|\n",
            "|  veh_1|  12.0|2023-09-13 18:32:33|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:34|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:36|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:38|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:40|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:41|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:42|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:45|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:46|     0|       0|                0|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:49|     0|       0|                0|\n",
            "|  veh_1|  13.0|2023-09-13 18:32:50|     0|       0|                0|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:52|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:56|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:58|     0|       0|                0|\n",
            "|  veh_1|  12.0|2023-09-13 18:33:00|     0|       0|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:33:02|     0|       0|                0|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+--------------+------------+-----------------------+\n",
            "|group_id|start_datetime|end_datetime|interval_length_samples|\n",
            "+--------+--------------+------------+-----------------------+\n",
            "+--------+--------------+------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = find_consecutive_intervals2(df,'signal','vehicle','datetime',10, n)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZnOp5Tksxp",
        "outputId": "1d38ab43-1bab-4b94-c3b2-26e622dae134"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|vehicle|signal|           datetime|is_v_0|group_id|consecutive_count|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|  veh_1|  10.0|2023-09-13 18:32:27|     1|       0|                1|\n",
            "|  veh_1|  13.0|2023-09-13 18:32:28|     0|       1|                0|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:31|     0|       2|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:32|     1|       2|                1|\n",
            "|  veh_1|  12.0|2023-09-13 18:32:33|     0|       3|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:34|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:36|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:38|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:40|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:41|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:42|     1|       3|                6|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:45|     1|       4|                2|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:46|     1|       4|                2|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:49|     0|       5|                0|\n",
            "|  veh_1|  13.0|2023-09-13 18:32:50|     0|       6|                0|\n",
            "|  veh_1|  11.0|2023-09-13 18:32:52|     0|       7|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:56|     1|       8|                2|\n",
            "|  veh_1|  10.0|2023-09-13 18:32:58|     1|       8|                2|\n",
            "|  veh_1|  12.0|2023-09-13 18:33:00|     0|      10|                0|\n",
            "|  veh_1|  10.0|2023-09-13 18:33:02|     1|      10|                1|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|vehicle|signal|           datetime|is_v_0|group_id|consecutive_count|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "|  veh_1|  10.0|2023-09-13 18:35:27|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:29|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:31|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:35|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:36|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:37|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:35:39|     1|      39|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:45|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:46|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:47|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:48|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:49|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:50|     1|     178|                7|\n",
            "|  veh_1|  10.0|2023-09-13 18:43:53|     1|     178|                7|\n",
            "|  veh_2|  10.0|2023-09-13 18:33:59|     1|      22|                8|\n",
            "|  veh_2|  10.0|2023-09-13 18:34:01|     1|      22|                8|\n",
            "|  veh_2|  10.0|2023-09-13 18:34:04|     1|      22|                8|\n",
            "|  veh_2|  10.0|2023-09-13 18:34:05|     1|      22|                8|\n",
            "|  veh_2|  10.0|2023-09-13 18:34:06|     1|      22|                8|\n",
            "|  veh_2|  10.0|2023-09-13 18:34:07|     1|      22|                8|\n",
            "+-------+------+-------------------+------+--------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+-------------------+-------------------+-----------------------+--------------+\n",
            "|group_id|     start_datetime|       end_datetime|interval_length_samples|first(vehicle)|\n",
            "+--------+-------------------+-------------------+-----------------------+--------------+\n",
            "|      22|2023-09-13 18:33:59|2023-09-13 18:34:09|                      8|         veh_2|\n",
            "|      26|2023-09-13 18:34:20|2023-09-13 18:34:27|                      7|         veh_2|\n",
            "|      39|2023-09-13 18:35:22|2023-09-13 18:35:39|                     18|         veh_1|\n",
            "|      49|2023-09-13 18:36:14|2023-09-13 18:36:30|                     13|         veh_2|\n",
            "|     153|2023-09-13 18:42:13|2023-09-13 18:42:22|                      8|         veh_2|\n",
            "|     169|2023-09-13 18:43:11|2023-09-13 18:43:21|                     10|         veh_2|\n",
            "|     178|2023-09-13 18:43:45|2023-09-13 18:43:53|                      7|         veh_1|\n",
            "|     218|2023-09-13 18:46:03|2023-09-13 18:46:12|                     10|         veh_2|\n",
            "+--------+-------------------+-------------------+-----------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}